{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e186b",
   "metadata": {},
   "source": [
    "Q4: Sentiment Analysis using HuggingFace Transformers\n",
    "\n",
    "Task: Use the HuggingFace transformers library to create a sentiment classifier. Your program should:\n",
    "\n",
    "- Load a pre-trained sentiment analysis pipeline\n",
    "- Analyze the following input sentence:\n",
    "  \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
    "\n",
    "Print:\n",
    "\n",
    "- Label (e.g., POSITIVE, NEGATIVE)\n",
    "- Confidence score (e.g., 0.9985)\n",
    "\n",
    "Expected Output:\n",
    "\n",
    "Your output should clearly display:\n",
    "\n",
    "Sentiment: [Label]\n",
    "\n",
    "Confidence Score: [Decimal between 0 and 1]\n",
    "\n",
    "Short Answer Questions:\n",
    "\n",
    "1. What is the main architectural difference between BERT and GPT? Which uses an encoder and which uses a decoder?\n",
    "2. Explain why using pre-trained models (like BERT or GPT) is beneficial for NLP applications instead of training from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7aa5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE\n",
      "Confidence Score: 0.9998\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Input sentence\n",
    "sentence = \"Despite the high price, the performance of the new MacBook is outstanding.\"\n",
    "\n",
    "# Analyze sentiment to get result\n",
    "result = sentiment_pipeline(sentence)[0]\n",
    "\n",
    "# Output results\n",
    "print(f\"Sentiment: {result['label']}\")\n",
    "print(f\"Confidence Score: {result['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb5995",
   "metadata": {},
   "source": [
    "Short Answer Questions:\n",
    "\n",
    "1. What is the main architectural difference between BERT and GPT? Which uses an encoder and which uses a decoder?\n",
    "\n",
    "- BERT stands for Bidirectional Encoder Representations from Transformers. It only uses an encoder and can understand the context of words from both left and right sides.\n",
    "- GPT is generative PRetrained Transformers and it only uses a decoder. It is used for generating information and it works from left to right. It is used to complete text and other tasks.\n",
    "\n",
    "2. Explain why using pre-trained models (like BERT or GPT) is beneficial for NLP applications instead of training from scratch.\n",
    "\n",
    "- Training models like GPT and BErT from scratch is a costly affair which needs lot of money and time for computing and training. So it is recommended to use already trained models. We can adjust or fine tune models to fit our purpose or task instead of training a model from scratch.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sttenvtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
